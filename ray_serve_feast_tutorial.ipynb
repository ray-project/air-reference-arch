{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be57a837",
   "metadata": {},
   "source": [
    "# Model Deployment on Ray Serve and Feast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff90c10",
   "metadata": {},
   "source": [
    "![Ray Serve and Feast](images/joint_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62554959",
   "metadata": {},
   "source": [
    "Welcome to this tutorial on model deployment on Ray Serve and Feast! This tutorial will show you how to\n",
    "\n",
    "1. Deploy your ML model on Ray Serve.\n",
    "2. Connect to your Feast feature store through a Ray Serve deployment.\n",
    "3. Separate your I/O-heavy business logic from your compute-heavy model inference for maximum scalability.\n",
    "\n",
    "If you have any questions about this tutorial, or any follow-up questions, please feel free to ask them in the [Ray discussion forum](https://discuss.ray.io/c/ray-serve/6) or the [Ray Slack](https://forms.gle/9TSdDYUgxYs8SA9e8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79488b59",
   "metadata": {},
   "source": [
    "# 1. Landscape of ML Serving Tools\n",
    "\n",
    "Where does Ray Serve fit in the landscape of machine learning serving tools? \n",
    "\n",
    "Generally speaking, there's a spectrum of ML serving tools:\n",
    "- People typically start with either framework-specific servers (e.g. TFServing, TorchServe) or generic web-serving frameworks (e.g. Flask, FastAPI) as an easy start to deploy a single model. \n",
    "- For more \"production-readiness\", people add custom toolings (e.g. Docker, K8s, Golang-based microservices). \n",
    "- But it's tough to maintain an ad-hoc pathwork of systems that are glued together. People are starting to look for special-purpose deployment tools (e.g. KubeFlow, KServe, Triton, etc.) to manage and deploy many models in production. \n",
    "\n",
    "In this spectrum of tools, users face tradeoffs between ease-of-development and production scalability. However, **Ray Serve lets you easily develop locally and then transparently scale to production**, giving you the best of both worlds!\n",
    "\n",
    "![Serve aims at both ease of development and ready for production.](images/serve_position.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec92b50",
   "metadata": {},
   "source": [
    "# 2. Feature Stores\n",
    "\n",
    "Features stores are systems that store your machine learning models' input data, also known as \"features.\" Managing data pipelines for these features can be challenging. Machine learning engineers and data scientists can integrate feature stores like Feast into their pipelines to aggregate, organize, store, and serve features. This can be especially helpful when considering that features may need to be collected in real time and stored in a way that makes them useful for both online inference and offline training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b109da",
   "metadata": {},
   "source": [
    "# 3. Example Model\n",
    "\n",
    "In this tutorial you will deploy a machine learning model locally via Ray Serve. The model will also access features from a Feast feature store.\n",
    "\n",
    "Let’s first take a look at how the model works, without using Ray Serve. Here's the the model and its supporting code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we specify metadata like the model checkpoint's filename\n",
    "# and the features we'll use in the model.\n",
    "\n",
    "MODEL_FILENAME = \"model.pkl\"\n",
    "ENCODER_FILENAME = \"encoder.bin\"\n",
    "\n",
    "categorical_features = [\n",
    "    \"person_home_ownership\",\n",
    "    \"loan_intent\",\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"location_type\",\n",
    "]\n",
    "\n",
    "feast_features = [\n",
    "    \"zipcode_features:city\",\n",
    "    \"zipcode_features:state\",\n",
    "    \"zipcode_features:location_type\",\n",
    "    \"zipcode_features:tax_returns_filed\",\n",
    "    \"zipcode_features:population\",\n",
    "    \"zipcode_features:total_wages\",\n",
    "    \"credit_history:credit_card_due\",\n",
    "    \"credit_history:mortgage_due\",\n",
    "    \"credit_history:student_loan_due\",\n",
    "    \"credit_history:vehicle_loan_due\",\n",
    "    \"credit_history:hard_pulls\",\n",
    "    \"credit_history:missed_payments_2y\",\n",
    "    \"credit_history:missed_payments_1y\",\n",
    "    \"credit_history:missed_payments_6m\",\n",
    "    \"credit_history:bankruptcies\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01d7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import feast\n",
    "\n",
    "from ray.ml.constants import MODEL_KEY as RAY_MODEL_KEY\n",
    "from ray.ml.checkpoint import Checkpoint as RayCheckpoint\n",
    "\n",
    "class CreditScoringModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Load model\n",
    "        print(\"Loading saved model...\\n\")\n",
    "        self.checkpoint = RayCheckpoint(MODEL_FILENAME)\n",
    "\n",
    "         # Load ordinal encoder\n",
    "        self.encoder = joblib.load(ENCODER_FILENAME)\n",
    "\n",
    "        # Set up Feast feature store\n",
    "        self.fs = feast.FeatureStore(repo_path=\"feature_repo\")\n",
    "\n",
    "    def predict(self, model_request):\n",
    "        # Preprocess the request\n",
    "        features_array = self.preprocess(model_request)\n",
    "        \n",
    "        # Make prediction\n",
    "        xgb_model = self._load_from_checkpoint(self.checkpoint)\n",
    "        prediction = xgb_model.inplace_predict(features_array)\n",
    "\n",
    "        # Return result of credit scoring\n",
    "        return np.round(prediction)\n",
    "    \n",
    "    def preprocess(self, model_request):\n",
    "        # Get online features from Feast\n",
    "        feature_vector = self._get_online_features_from_feast(model_request)\n",
    "\n",
    "        # Join features to request features\n",
    "        features = model_request.copy()\n",
    "        features.update(feature_vector)\n",
    "        features_df = pd.DataFrame.from_dict(features)\n",
    "\n",
    "        # Apply ordinal encoding to categorical features\n",
    "        self._apply_ordinal_encoding(features_df)\n",
    "\n",
    "        # Sort columns\n",
    "        features_df = features_df.reindex(sorted(features_df.columns), axis=1)\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        features_df = features_df[features_df.columns.drop(\"zipcode\").drop(\"dob_ssn\")]\n",
    "        print(\"================================\")\n",
    "        print(f\"Inference request:\")\n",
    "        print(\"--------------------------------\")\n",
    "        print(features_df.T)\n",
    "        print(\"================================\\n\")\n",
    "        return np.array(features_df)\n",
    "    \n",
    "    def _apply_ordinal_encoding(self, requests):\n",
    "        requests[categorical_features] = self.encoder.transform(\n",
    "            requests[categorical_features]\n",
    "        )\n",
    "    \n",
    "    def _get_online_features_from_feast(self, request):\n",
    "        zipcode = request[\"zipcode\"][0]\n",
    "        dob_ssn = request[\"dob_ssn\"][0]\n",
    "\n",
    "        return self.fs.get_online_features(\n",
    "            entity_rows=[{\"zipcode\": zipcode, \"dob_ssn\": dob_ssn}],\n",
    "            features=feast_features,\n",
    "        ).to_dict()\n",
    "    \n",
    "    def _load_from_checkpoint(self, checkpoint: RayCheckpoint):\n",
    "        checkpoint_path = checkpoint.to_directory()\n",
    "        xgb_model = xgb.Booster()\n",
    "        xgb_model.load_model(os.path.join(checkpoint_path, RAY_MODEL_KEY))\n",
    "        return xgb_model\n",
    "\n",
    "model = CreditScoringModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4550e",
   "metadata": {},
   "source": [
    "The `CreditScoringModel` recommends loan approval or rejection using features that include info such as income and interest rates. During initialization, it loads a pre-trained model using the Ray checkpoint stored at `MODEL_FILENAME`. When it receives an inference request, it combines the request data with online features that it retrieves from a Feast feature store. Then, it runs its `predict()` method, which preprocesses the request and then runs inference on it. Based on the result, the code prints whether a loan should be approved or rejected. We can run a sample request to verify that the model works correctly locally. This request should print `Loan rejected!`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf7e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_request = {\n",
    "    \"zipcode\": [76104],\n",
    "    \"dob_ssn\": [\"19630621_4278\"],\n",
    "    \"person_age\": [133],\n",
    "    \"person_income\": [59000],\n",
    "    \"person_home_ownership\": [\"RENT\"],\n",
    "    \"person_emp_length\": [123.0],\n",
    "    \"loan_intent\": [\"PERSONAL\"],\n",
    "    \"loan_amnt\": [35000],\n",
    "    \"loan_int_rate\": [16.02],\n",
    "}\n",
    "\n",
    "result = model.predict(loan_request)[0]\n",
    "print(f\"Inference result: {result}\\n\")\n",
    "\n",
    "if result == 0:\n",
    "    print(\"Loan approved!\")\n",
    "elif result == 1:\n",
    "    print(\"Loan rejected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e6144",
   "metadata": {},
   "source": [
    "# 4. Converting to a Ray Serve Deployment \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7daad8c",
   "metadata": {},
   "source": [
    "This tutorial’s goal is to deploy this model using Ray Serve, so it can be scaled up and queried over HTTP. We’ll start by converting the `CreditScoringModel` Python class into a Ray Serve deployment that can be launched locally on a laptop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b22f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import feast\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "from ray.ml.constants import MODEL_KEY as RAY_MODEL_KEY\n",
    "from ray.ml.checkpoint import Checkpoint as RayCheckpoint\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class CreditScoringModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Load model\n",
    "        print(\"Loading saved model...\\n\")\n",
    "        self.checkpoint = RayCheckpoint(MODEL_FILENAME)\n",
    "\n",
    "         # Load ordinal encoder\n",
    "        self.encoder = joblib.load(ENCODER_FILENAME)\n",
    "\n",
    "        # Set up Feast feature store\n",
    "        self.fs = feast.FeatureStore(repo_path=\"feature_repo\")\n",
    "    \n",
    "    async def __call__(self, http_request):\n",
    "        body = await http_request.json()\n",
    "        return self.predict(body)\n",
    "\n",
    "    def predict(self, model_request):\n",
    "        # Preprocess the request\n",
    "        features_array = self.preprocess(model_request)\n",
    "        \n",
    "        # Make prediction\n",
    "        xgb_model = self._load_from_checkpoint(self.checkpoint)\n",
    "        prediction = xgb_model.inplace_predict(features_array)\n",
    "\n",
    "        # Return result of credit scoring\n",
    "        return np.round(prediction)\n",
    "    \n",
    "    def preprocess(self, model_request):\n",
    "        # Get online features from Feast\n",
    "        feature_vector = self._get_online_features_from_feast(model_request)\n",
    "\n",
    "        # Join features to request features\n",
    "        features = model_request.copy()\n",
    "        features.update(feature_vector)\n",
    "        features_df = pd.DataFrame.from_dict(features)\n",
    "\n",
    "        # Apply ordinal encoding to categorical features\n",
    "        self._apply_ordinal_encoding(features_df)\n",
    "\n",
    "        # Sort columns\n",
    "        features_df = features_df.reindex(sorted(features_df.columns), axis=1)\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        features_df = features_df[features_df.columns.drop(\"zipcode\").drop(\"dob_ssn\")]\n",
    "        print(\"================================\")\n",
    "        print(f\"Inference request:\")\n",
    "        print(\"--------------------------------\")\n",
    "        print(features_df.T)\n",
    "        print(\"================================\\n\")\n",
    "        return np.array(features_df)\n",
    "    \n",
    "    def _apply_ordinal_encoding(self, requests):\n",
    "        requests[categorical_features] = self.encoder.transform(\n",
    "            requests[categorical_features]\n",
    "        )\n",
    "    \n",
    "    def _get_online_features_from_feast(self, request):\n",
    "        zipcode = request[\"zipcode\"][0]\n",
    "        dob_ssn = request[\"dob_ssn\"][0]\n",
    "\n",
    "        return self.fs.get_online_features(\n",
    "            entity_rows=[{\"zipcode\": zipcode, \"dob_ssn\": dob_ssn}],\n",
    "            features=feast_features,\n",
    "        ).to_dict()\n",
    "    \n",
    "    def _load_from_checkpoint(self, checkpoint: RayCheckpoint):\n",
    "        checkpoint_path = checkpoint.to_directory()\n",
    "        xgb_model = xgb.Booster()\n",
    "        xgb_model.load_model(os.path.join(checkpoint_path, RAY_MODEL_KEY))\n",
    "        return xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4409189",
   "metadata": {},
   "source": [
    "Notice that there were minimal code changes to convert this `CreditScoringModel` into a Ray Serve deployment. In fact, only two changes were made:\n",
    "\n",
    "1. We added the `@serve.deployment` decorator, which converts the class into a Ray Serve `Deployment` object. This object can later be deployed using the `deploy()` method.\n",
    "2. Then, we added a `__call__()` method. This is the method that will receive HTTP requests to our model. It unpacks the request's JSON body, and it calls the `CreditScoringModel`'s `predict()` method, which has no code changes.\n",
    "\n",
    "Next, we can start a local Ray cluster (with `ray.init()`), and a Serve application (with `serve.start()`) on top of it. After that, we can deploy `CreditScoringModel`. First we make sure to shutdown any lingering Ray cluster that may already exist in the background from previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(runtime_env={\"working_dir\": \".\"})\n",
    "serve.start()\n",
    "\n",
    "\n",
    "CreditScoringModel.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b3b95",
   "metadata": {},
   "source": [
    "**Note:** the `runtime_env={\"working_dir\": \".\"}` parameter that's passed into `ray.init()` packages this notebook's directory and uploads it to the Ray cluster. This allows `CreditScoringModel` to access the checkpoint files stored in this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d80fd2",
   "metadata": {},
   "source": [
    "# 5. Testing the Ray Serve Deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1471a5",
   "metadata": {},
   "source": [
    "Now that the `CreditScoringModel` is running in the Ray cluster as a Ray Serve deployment, we can query it over HTTP. We can use the Python `requests` library to send a POST request to the deployment, containing the same `loan_request` from our local test. We should again expect this program to print `Loan rejected!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec2fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "loan_request = {\n",
    "    \"zipcode\": [76104],\n",
    "    \"dob_ssn\": [\"19630621_4278\"],\n",
    "    \"person_age\": [133],\n",
    "    \"person_income\": [59000],\n",
    "    \"person_home_ownership\": [\"RENT\"],\n",
    "    \"person_emp_length\": [123.0],\n",
    "    \"loan_intent\": [\"PERSONAL\"],\n",
    "    \"loan_amnt\": [35000],\n",
    "    \"loan_int_rate\": [16.02],\n",
    "}\n",
    "\n",
    "response = requests.post(\"http://localhost:8000/CreditScoringModel\", json=loan_request)\n",
    "result = response.json()[0]\n",
    "print(f\"Inference result: {result}\\n\")\n",
    "\n",
    "if result == 0:\n",
    "    print(\"Loan approved!\\n\")\n",
    "elif result == 1:\n",
    "    print(\"Loan rejected!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd3b5b",
   "metadata": {},
   "source": [
    "# 6. Increasing Scalability: Splitting Business Logic and ML Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3042db",
   "metadata": {},
   "source": [
    "Ray Serve allows you to easily scale your deployments using the `num_replicas` paramter. This parameter can be set in the `@serve.deployment` decorator, and it controls how many copies of the deployment process exists in the cluster. A higher number of `num_replicas` allows your deployment to serve more requests.\n",
    "\n",
    "However, note that our model has two distinct steps: (1) preprocessing and (2) ML inference. Preprocessing is an I/O-heavy task, requiring communication with the feature store. ML inference is a compute-heavy task, requiring more CPU/GPU time compared to preprocessing. If our model becomes bottlenecked because of (1), then when we scale it, we waste CPU/GPU resources that get reserved by the extra replicas. On the other hand, if it's bottlenecked because of (2), then scaling it wastes memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b186c35b",
   "metadata": {},
   "source": [
    "<img src=\"images/resource_bounds.png\" alt=\"Disparate resource consumption\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddca8b9",
   "metadata": {},
   "source": [
    "Ray Serve's flexible API makes it easy to split business logic (i.e. anything that's not the ML inference itself), such as preprocessing, from the ML inference step. By splitting these steps into different deployments, we gain finer-grained control over our deployment's scaling and resource consumption. We can split steps (1) and (2), so when we scale one deployment, we don't waste resources scaling the other. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602dff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import feast\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "from ray.ml.constants import MODEL_KEY as RAY_MODEL_KEY\n",
    "from ray.ml.checkpoint import Checkpoint as RayCheckpoint\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class CreditScoringPreprocessor:\n",
    "\n",
    "    def __init__(self):\n",
    "         # Load ordinal encoder\n",
    "        self.encoder = joblib.load(ENCODER_FILENAME)\n",
    "\n",
    "        # Set up Feast feature store\n",
    "        self.fs = feast.FeatureStore(repo_path=\"feature_repo\")\n",
    "    \n",
    "    async def __call__(self, http_request):\n",
    "        model_request = await http_request.json()\n",
    "        features_array = self.preprocess(model_request)\n",
    "        model_handle = serve.get_deployment(\"CreditScoringModel\").get_handle()\n",
    "        return ray.get(model_handle.predict.remote(features_array))\n",
    "    \n",
    "    def preprocess(self, model_request):\n",
    "        # Get online features from Feast\n",
    "        feature_vector = self._get_online_features_from_feast(model_request)\n",
    "\n",
    "        # Join features to request features\n",
    "        features = model_request.copy()\n",
    "        features.update(feature_vector)\n",
    "        features_df = pd.DataFrame.from_dict(features)\n",
    "\n",
    "        # Apply ordinal encoding to categorical features\n",
    "        self._apply_ordinal_encoding(features_df)\n",
    "\n",
    "        # Sort columns\n",
    "        features_df = features_df.reindex(sorted(features_df.columns), axis=1)\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        features_df = features_df[features_df.columns.drop(\"zipcode\").drop(\"dob_ssn\")]\n",
    "        print(\"================================\")\n",
    "        print(f\"Inference request:\")\n",
    "        print(\"--------------------------------\")\n",
    "        print(features_df.T)\n",
    "        print(\"================================\\n\")\n",
    "        return np.array(features_df)\n",
    "    \n",
    "    def _apply_ordinal_encoding(self, requests):\n",
    "        requests[categorical_features] = self.encoder.transform(\n",
    "            requests[categorical_features]\n",
    "        )\n",
    "    \n",
    "    def _get_online_features_from_feast(self, request):\n",
    "        zipcode = request[\"zipcode\"][0]\n",
    "        dob_ssn = request[\"dob_ssn\"][0]\n",
    "\n",
    "        return self.fs.get_online_features(\n",
    "            entity_rows=[{\"zipcode\": zipcode, \"dob_ssn\": dob_ssn}],\n",
    "            features=feast_features,\n",
    "        ).to_dict()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class CreditScoringModel:\n",
    "    MODEL_FILENAME = \"model.pkl\"\n",
    "    ENCODER_FILENAME = \"encoder.bin\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Load model\n",
    "        print(\"Loading saved model...\")\n",
    "        self.checkpoint = ray.ml.checkpoint.Checkpoint(MODEL_FILENAME)\n",
    "\n",
    "         # Load ordinal encoder\n",
    "        self.encoder = joblib.load(ENCODER_FILENAME)\n",
    "\n",
    "    def predict(self, features_array):\n",
    "        # Make prediction\n",
    "        print(\"Running inference...\")\n",
    "        xgb_model = self._load_from_checkpoint(self.checkpoint)\n",
    "        prediction = xgb_model.inplace_predict(features_array)\n",
    "\n",
    "        # Return result of credit scoring\n",
    "        return np.round(prediction)\n",
    "    \n",
    "    def _load_from_checkpoint(self, checkpoint: RayCheckpoint):\n",
    "        checkpoint_path = checkpoint.to_directory()\n",
    "        xgb_model = xgb.Booster()\n",
    "        xgb_model.load_model(os.path.join(checkpoint_path, RAY_MODEL_KEY))\n",
    "        return xgb_model\n",
    "\n",
    "\n",
    "CreditScoringModel.deploy()\n",
    "CreditScoringPreprocessor.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5a568",
   "metadata": {},
   "source": [
    "In this setup, `CreditScorePreprocessor` receives the HTTP queries. It preprocesses the request and then forwards it to `CreditScoreModel`, which runs inference. `CreditScorePreprocessor` accesses `CreditScoreModel` using a **Ray Serve Handle**:\n",
    "\n",
    "```python\n",
    "model_handle = serve.get_deployment(\"CreditScoringModel\").get_handle()\n",
    "return ray.get(model_handle.predict.remote(features_array))\n",
    "```\n",
    "\n",
    "This handle lets you send requests between deployments Pythonically, as though you were passing requests directly between functions or classes. This makes for a more intuitive development and debugging process. This handle also enables straightforward **model composition**, a workload well-suited for Ray Serve. Model composition involves request being routed between multiple different ML models. This is a common pattern when developing aggregation pipelines or ensembles models. Instead of packaging each model in a separate microservice, Ray Serve allows you to develop and deploy them through a single codebase. This makes it easier for you to reason about your multi-model workloads and better understand how the different models interact.\n",
    "\n",
    "Here are a few common multi-model patterns found in industry:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb74ca0",
   "metadata": {},
   "source": [
    "![Model composition in industry](images/model_composition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ac15d",
   "metadata": {},
   "source": [
    "We can test our new deployment setup using the same request. However, this time we'll direct our request to the `CreditScoringPreprocessor` endpoint since that handles preprocessing and calling the model's `predict()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62382e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "loan_request = {\n",
    "    \"zipcode\": [76104],\n",
    "    \"dob_ssn\": [\"19630621_4278\"],\n",
    "    \"person_age\": [133],\n",
    "    \"person_income\": [59000],\n",
    "    \"person_home_ownership\": [\"RENT\"],\n",
    "    \"person_emp_length\": [123.0],\n",
    "    \"loan_intent\": [\"PERSONAL\"],\n",
    "    \"loan_amnt\": [35000],\n",
    "    \"loan_int_rate\": [16.02],\n",
    "}\n",
    "\n",
    "response = requests.post(\"http://localhost:8000/CreditScoringPreprocessor\", json=loan_request)\n",
    "result = response.json()[0]\n",
    "print(f\"Inference result: {result}\\n\")\n",
    "\n",
    "if result == 0:\n",
    "    print(\"Loan approved!\\n\")\n",
    "elif result == 1:\n",
    "    print(\"Loan rejected!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce98e9",
   "metadata": {},
   "source": [
    "# 7. FastAPI Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a0e18",
   "metadata": {},
   "source": [
    "Ray Serve provides native FastAPI integration that allows you to scale your pre-existing FastAPI applications. The integations also allows you to extend you Ray Serve application with FastAPI, which provides some nice benefits, including automatic parameter validation, some HTTP preprocessing, and a UI to query your deployments. Let's adapt our Ray Serve deployments to use FastAPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pydantic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from fastapi import FastAPI\n",
    "from typing import List\n",
    "\n",
    "import feast\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "from ray.ml.constants import MODEL_KEY as RAY_MODEL_KEY\n",
    "from ray.ml.checkpoint import Checkpoint as RayCheckpoint\n",
    "\n",
    "MODEL_FILENAME = \"model.pkl\"\n",
    "ENCODER_FILENAME = \"encoder.bin\"\n",
    "\n",
    "categorical_features = [\n",
    "    \"person_home_ownership\",\n",
    "    \"loan_intent\",\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"location_type\",\n",
    "]\n",
    "\n",
    "feast_features = [\n",
    "    \"zipcode_features:city\",\n",
    "    \"zipcode_features:state\",\n",
    "    \"zipcode_features:location_type\",\n",
    "    \"zipcode_features:tax_returns_filed\",\n",
    "    \"zipcode_features:population\",\n",
    "    \"zipcode_features:total_wages\",\n",
    "    \"credit_history:credit_card_due\",\n",
    "    \"credit_history:mortgage_due\",\n",
    "    \"credit_history:student_loan_due\",\n",
    "    \"credit_history:vehicle_loan_due\",\n",
    "    \"credit_history:hard_pulls\",\n",
    "    \"credit_history:missed_payments_2y\",\n",
    "    \"credit_history:missed_payments_1y\",\n",
    "    \"credit_history:missed_payments_6m\",\n",
    "    \"credit_history:bankruptcies\",\n",
    "]\n",
    "\n",
    "class LoanRequest(pydantic.BaseModel):\n",
    "    zipcode: List[int]\n",
    "    dob_ssn: List[str]\n",
    "    person_age: List[int]\n",
    "    person_income: List[int]\n",
    "    person_home_ownership: List[str]\n",
    "    person_emp_length: List[float]\n",
    "    loan_intent: List[str]\n",
    "    loan_amnt: List[int]\n",
    "    loan_int_rate: List[float]\n",
    "\n",
    "class LoanDecision(pydantic.BaseModel):\n",
    "    decision: float\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class CreditScoringPreprocessor:\n",
    "\n",
    "    def __init__(self):\n",
    "         # Load ordinal encoder\n",
    "        self.encoder = joblib.load(ENCODER_FILENAME)\n",
    "\n",
    "        # Set up Feast feature store\n",
    "        self.fs = feast.FeatureStore(repo_path=\"feature_repo\")\n",
    "    \n",
    "    @app.post(\"/\", response_model=LoanDecision)\n",
    "    async def route(self, loan_request: LoanRequest):\n",
    "        features_array = self.preprocess(loan_request.dict())\n",
    "        model_handle = serve.get_deployment(\"CreditScoringModel\").get_handle()\n",
    "        a = ray.get(model_handle.predict.remote(features_array))\n",
    "        return {\"decision\": a}\n",
    "    \n",
    "    def preprocess(self, model_request):\n",
    "        # Get online features from Feast\n",
    "        feature_vector = self._get_online_features_from_feast(model_request)\n",
    "\n",
    "        # Join features to request features\n",
    "        features = model_request.copy()\n",
    "        features.update(feature_vector)\n",
    "        features_df = pd.DataFrame.from_dict(features)\n",
    "\n",
    "        # Apply ordinal encoding to categorical features\n",
    "        self._apply_ordinal_encoding(features_df)\n",
    "\n",
    "        # Sort columns\n",
    "        features_df = features_df.reindex(sorted(features_df.columns), axis=1)\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        features_df = features_df[features_df.columns.drop(\"zipcode\").drop(\"dob_ssn\")]\n",
    "        print(\"================================\")\n",
    "        print(f\"Inference request:\")\n",
    "        print(\"--------------------------------\")\n",
    "        print(features_df.T)\n",
    "        print(\"================================\\n\")\n",
    "        return np.array(features_df)\n",
    "    \n",
    "    def _apply_ordinal_encoding(self, requests):\n",
    "        requests[categorical_features] = self.encoder.transform(\n",
    "            requests[categorical_features]\n",
    "        )\n",
    "    \n",
    "    def _get_online_features_from_feast(self, request):\n",
    "        zipcode = request[\"zipcode\"][0]\n",
    "        dob_ssn = request[\"dob_ssn\"][0]\n",
    "\n",
    "        return self.fs.get_online_features(\n",
    "            entity_rows=[{\"zipcode\": zipcode, \"dob_ssn\": dob_ssn}],\n",
    "            features=feast_features,\n",
    "        ).to_dict()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class CreditScoringModel:\n",
    "    MODEL_FILENAME = \"model.pkl\"\n",
    "    ENCODER_FILENAME = \"encoder.bin\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Load model\n",
    "        print(\"Loading saved model...\")\n",
    "        self.checkpoint = ray.ml.checkpoint.Checkpoint(MODEL_FILENAME)\n",
    "\n",
    "         # Load ordinal encoder\n",
    "        self.encoder = joblib.load(ENCODER_FILENAME)\n",
    "\n",
    "    def predict(self, features_array):\n",
    "        # Make prediction\n",
    "        print(\"Running inference...\")\n",
    "        xgb_model = self._load_from_checkpoint(self.checkpoint)\n",
    "        prediction = xgb_model.inplace_predict(features_array)\n",
    "\n",
    "        # Return result of credit scoring\n",
    "        return np.round(prediction)\n",
    "    \n",
    "    def _load_from_checkpoint(self, checkpoint: RayCheckpoint):\n",
    "        checkpoint_path = checkpoint.to_directory()\n",
    "        xgb_model = xgb.Booster()\n",
    "        xgb_model.load_model(os.path.join(checkpoint_path, RAY_MODEL_KEY))\n",
    "        return xgb_model\n",
    "\n",
    "ray.init(runtime_env={\"working_dir\": \".\"})\n",
    "serve.start()\n",
    "\n",
    "CreditScoringModel.deploy()\n",
    "CreditScoringPreprocessor.deploy()\n",
    "\n",
    "\n",
    "# *************************************************************************************\n",
    "\n",
    "\n",
    "# Request that gets loan rejected:\n",
    "\n",
    "# loan_request = {\n",
    "#     \"zipcode\": [76104],\n",
    "#     \"dob_ssn\": [\"19630621_4278\"],\n",
    "#     \"person_age\": [133],\n",
    "#     \"person_income\": [59000],\n",
    "#     \"person_home_ownership\": [\"RENT\"],\n",
    "#     \"person_emp_length\": [123.0],\n",
    "#     \"loan_intent\": [\"PERSONAL\"],\n",
    "#     \"loan_amnt\": [35000],\n",
    "#     \"loan_int_rate\": [16.02],\n",
    "# }\n",
    "\n",
    "# Interactive docs: http://localhost:8000/CreditScoringPreprocessor/docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b46c058",
   "metadata": {},
   "source": [
    "# 8. Additional Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63008433",
   "metadata": {},
   "source": [
    "Congratulations on finishing this notebook! If you're interested in learning more, check out these additional resources:\n",
    "\n",
    "* [Ray Serve Documentation](rayserve.org)\n",
    "* [Ray Homepage](ray.io)\n",
    "* [Ray Documentation](https://docs.ray.io/en/latest/)\n",
    "\n",
    "Also, join the Ray community at the [Ray discussion forum](https://discuss.ray.io/c/ray-serve/6) and the [Ray Slack](https://forms.gle/9TSdDYUgxYs8SA9e8)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
